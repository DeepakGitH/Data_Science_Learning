##### Week 2 Lectures ######

1. Video 1 - It is easir in NN implementations to have the matrix with columns as data points and rows with features.
2. Video 3 - Loss function is for an example while Cost function is for the entire training set
3. Video 3 - The cost function least squares is not convex for a classification task.
4. Video 4 - Cost function = -1/m * Cross Entropy
5. Computation Graph - Helps in visualizing the forward and backward pas. If you are doing a backward pass it is always better to do a
computation graph
6. In gradient descent the number of epochs (iterations) are on loop but the calculations on data is always vectorised
7. Numpy vector and matrix dimensions are very fast and should always be used.
  - SIMD = Single Instruction Multiple Data
  Without using any for loop you can vectorize the entire data set
8. Better to use matrices than "rank array" in numpy
9. Brodcasting in numpy 
10. ASSERT is a good method to check the output from your code

######## Week 3 Lectures #######

1. Vectorized implementation of NN is very important for understanding. Each Weight W is a matrix with rows representing the number
of nodes in the Hidden Layer and Columns representing the weights for each neuron coming from the nodes in the previous layer. B is 
number of nodes in the hidden layer. The output after the linear transformation is also in the same form. The number of columns in the Z 
matrix is the number of data points in training and number of rows is the number of nodes in the hidden layer.
2. Hyperbolic works better than the sigmoid because it makes the output more centered.
3. In practice in computers the exact zero will never occur so you never have to worry that ReLu is not differentiable at zero
4. Leaky Relu works better
5. Without non linearity hidden layers are just useless
6. If the weights are not initialized randomly we will see all the same updates in the hidden layer to be the same because of the 
symmetry. We should initialize the weights to small values.

########### Week 4 Lectures ################
1. THinking about the matrix is the best way to debug your code.
2. Even in audio, it is audio -> Phonems -> Words -> Sentences
